{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI6XS5ssXDp/t+YNkQ6SLc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayOfLight07/Deep-Learning/blob/main/DL_LAB_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning Lab_________________________________________________________________________Name: Deepasnhu Rathore     \n",
        "B.Tech. 5th Sem.\n",
        "#**Experiment:- 11**\n",
        "# Image Reconstruction and Feature Compression Using Autoencoders\n",
        "\n",
        "Date:18\\11\\2025____________________________________________________________________________________SAP: 500124406"
      ],
      "metadata": {
        "id": "HOoA1-e9r6dc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b16921c3"
      },
      "source": [
        "# **AIM**\n",
        "\n",
        "**Aim**: To implement a standard Autoencoder using TensorFlow/Keras to perform image reconstruction on the MNIST handwritten digit dataset.\n",
        "\n",
        "# **Objective**\n",
        "\n",
        "*   To understand the Encoder-Decoder architecture.\n",
        "\n",
        "*   To analyze how deep neural networks can perform dimensionality reduction (feature compression) by learning a compressed latent representation.\n",
        "\n",
        "*   To evaluate the quality of reconstructed images compared to original inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ae6441"
      },
      "source": [
        "# **Theory**\n",
        "\n",
        "An Autoencoder is a type of unsupervised artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction.\n",
        "\n",
        "It consists of two main parts:\n",
        "\n",
        "**Encoder:** Maps the input $x$ to a lower-dimensional feature vector (latent space representation) $h$. This forces the network to learn the most important features and discard noise.\n",
        "$$h = f(x)$$\n",
        "\n",
        "**Decoder:** Maps the latent space representation $h$ back to a reconstruction of the input $\\hat{x}$.\n",
        "$$\\hat{x} = g(h)$$\n",
        "\n",
        "The network is trained to minimize the reconstruction error (Loss), usually Mean Squared Error (MSE) or Binary Crossentropy, between the input $x$ and the output $\\hat{x}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "269b072e"
      },
      "source": [
        "## **Algorithm:**\n",
        "\n",
        "\n",
        "1.   Input: Load the MNIST dataset (28x28 pixel images).\n",
        "2.   Preprocessing:\n",
        "\n",
        "     *   Normalize pixel values to the range [0, 1].\n",
        "\n",
        "     *   Flatten the 2D images (28x28) into 1D vectors (784 dimensions).\n",
        "\n",
        "\n",
        "\n",
        "3.   Model Architecture:\n",
        "\n",
        "     *   Input Layer: 784 neurons.\n",
        "\n",
        "     *   Encoding Layer (Bottleneck): Reduce dimensionality (e.g., to 32 neurons). This represents the compressed features.\n",
        "\n",
        "     *   Decoding Layer: Expand dimensionality back to 784 neurons.\n",
        "\n",
        "\n",
        "\n",
        "4.   Training: Train the model using x_train as both the input and the target (Self-supervised learning).\n",
        "\n",
        "5.   Evaluation: Pass test images through the trained autoencoder and visualize the reconstructed output alongside the original image.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hFox8x4HCkd",
        "outputId": "a150096e-b734-4f39-d11a-742b2a5a72d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and Preprocess Data**\n",
        "This cell loads the MNIST dataset, normalizes the pixel values (0 to 1), and flattens the 2D images into 1D vectors."
      ],
      "metadata": {
        "id": "BGS0QEJaLdp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Flatten the images:\n",
        "# Original shape: (60000, 28, 28) -> New shape: (60000, 784)\n",
        "x_train = x_train.reshape((len(x_train), 784))\n",
        "x_test = x_test.reshape((len(x_test), 784))\n",
        "\n",
        "print(f\"Training Data Shape: {x_train.shape}\")\n",
        "print(f\"Testing Data Shape: {x_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHxArkC0LYmd",
        "outputId": "ec922e2b-52bc-4ae7-976f-a579d870eac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Training Data Shape: (60000, 784)\n",
            "Testing Data Shape: (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define the Autoencoder Model**\n",
        "This cell defines the architecture. We compress the 784 inputs down to 32 neurons (Encoder), and then expand them back to 784 (Decoder)."
      ],
      "metadata": {
        "id": "nypKrHm8MO9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimension of the latent space (compressed feature)\n",
        "encoding_dim = 32\n",
        "\n",
        "# Input placeholder\n",
        "input_img = tf.keras.Input(shape=(784,))\n",
        "\n",
        "# Encoder Layer\n",
        "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
        "\n",
        "# Decoder Layer\n",
        "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# Autoencoder Model (Input -> Reconstructed Output)\n",
        "autoencoder = models.Model(input_img, decoded)\n",
        "\n",
        "# Encoder Model (Input -> Encoded Representation) - useful for visualization\n",
        "encoder = models.Model(input_img, encoded)\n",
        "\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "HH3e7XoIMUBa",
        "outputId": "17db1801-07c0-4172-a825-cd69dc638ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m25,120\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │        \u001b[38;5;34m25,872\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,120</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,872</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,992\u001b[0m (199.19 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,992</span> (199.19 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,992\u001b[0m (199.19 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,992</span> (199.19 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compile and Train**\n",
        "This takes about 1-2 minutes. We use x_train as both input and target because this is unsupervised learning."
      ],
      "metadata": {
        "id": "m55uWY8CMXPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "history = autoencoder.fit(x_train, x_train,\n",
        "                epochs=10,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shKDh08lMbmr",
        "outputId": "c1a60ac1-a12d-429c-c49d-88fec5addba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.3853 - val_loss: 0.1877\n",
            "Epoch 2/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.1781 - val_loss: 0.1522\n",
            "Epoch 3/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.1482 - val_loss: 0.1333\n",
            "Epoch 4/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.1314 - val_loss: 0.1211\n",
            "Epoch 5/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.1198 - val_loss: 0.1121\n",
            "Epoch 6/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.1118 - val_loss: 0.1061\n",
            "Epoch 7/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.1058 - val_loss: 0.1016\n",
            "Epoch 8/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.1020 - val_loss: 0.0986\n",
            "Epoch 9/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0993 - val_loss: 0.0966\n",
            "Epoch 10/10\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0974 - val_loss: 0.0951\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualize Results and Loss**\n",
        "This cell generates the comparison images (Original vs. Reconstructed) and plots the training loss graph."
      ],
      "metadata": {
        "id": "qC4RV8E_Me4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Generate reconstructions\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "# 2. Plot Original vs Reconstructed Images\n",
        "n = 10  # Number of digits to display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display Original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    if i == n//2:\n",
        "        ax.set_title('Original Images')\n",
        "\n",
        "    # Display Reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    if i == n//2:\n",
        "        ax.set_title('Reconstructed Images')\n",
        "plt.show()\n",
        "\n",
        "# 3. Plot Training Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss during Training')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bQnOjNnxMluT",
        "outputId": "9649047b-8c02-4f5f-cf04-1f31bec93e27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m 42/313\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36d55c00"
      },
      "source": [
        "## **Observations**\n",
        "\n",
        "*   **Loss Convergence:** The Binary Crossentropy loss decreased steadily over 10 epochs, indicating the model successfully learned to approximate the identity function.\n",
        "\n",
        "*   **Compression:** The input dimension of 784 was compressed to a latent dimension of 32. This is a compression ratio of approximately 24:1.\n",
        "\n",
        "*   **Visual Quality:** As seen in the generated plot, the reconstructed digits are slightly blurrier than the originals (due to information loss during compression) but are fully recognizable and retain the structural integrity of the digits.\n",
        "\n",
        "# **Conclusion**\n",
        "\n",
        " In this experiment, we successfully designed and evaluated a standard Autoencoder. The model demonstrated that it is possible to compress data significantly into a lower-dimensional latent space and reconstruct it with high fidelity. This confirms the utility of Autoencoders for feature extraction and data compression tasks."
      ]
    }
  ]
}